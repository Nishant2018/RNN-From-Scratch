{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b442669",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.006162,
     "end_time": "2024-06-10T08:38:05.684416",
     "exception": false,
     "start_time": "2024-06-10T08:38:05.678254",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"padding:10px; \n",
    "            color:#FF9F00;\n",
    "            margin:10px;\n",
    "            font-size:150%;\n",
    "            display:fill;\n",
    "            border-radius:1px;\n",
    "            border-style: solid;\n",
    "            border-color:#FF9F00;\n",
    "            background-color:#3E3D53;\n",
    "            overflow:hidden;\">\n",
    "    <center>\n",
    "        <a id='top'></a>\n",
    "        <b>Table of Contents</b>\n",
    "    </center>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>\n",
    "            <a href=\"#1\">1 -  Overview and Imports</a>\n",
    "        </li>\n",
    "        <li>\n",
    "            <a href=\"#2\">2 - Data Preparation</a>\n",
    "        </li>\n",
    "        <li>\n",
    "            <a href=\"#3\">3 - RNN Implementation</a>\n",
    "        <li>\n",
    "            <a href=\"#4\">4 - Thank you</a>\n",
    "        </li> \n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "\n",
    " \n",
    "    \n",
    " <a id=\"1\"></a>\n",
    "<h1 style='background:#FF9F00;border:0; color:black;\n",
    "    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n",
    "    transform: rotateX(10deg);\n",
    "    '><center style='color: #3E3D53;'>Overview and Imports</center></h1>\n",
    "\n",
    "# Overview and Imports\n",
    "**Recurrent Neural Networks (RNNs) are a type of neural network that are designed to handle sequential data, such as time series or natural language text. They achieve this by using a hidden state that is updated for each time step of the input sequence, allowing the network to maintain a memory of previous inputs.**\n",
    " \n",
    "\n",
    "**This notebook contains an implementation of an RNN that can be used for language modeling. The self takes in a sequence of characters and outputs the probability distribution over the next character in the sequence. The network is trained on a corpus of text and then used to generate new text that has a similar distribution of characters as the training corpus.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ed981f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T08:38:05.699179Z",
     "iopub.status.busy": "2024-06-10T08:38:05.698263Z",
     "iopub.status.idle": "2024-06-10T08:38:05.736400Z",
     "shell.execute_reply": "2024-06-10T08:38:05.735100Z"
    },
    "papermill": {
     "duration": 0.048712,
     "end_time": "2024-06-10T08:38:05.739413",
     "exception": false,
     "start_time": "2024-06-10T08:38:05.690701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9982b28",
   "metadata": {
    "papermill": {
     "duration": 0.005154,
     "end_time": "2024-06-10T08:38:05.750274",
     "exception": false,
     "start_time": "2024-06-10T08:38:05.745120",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " <a id=\"2\"></a>\n",
    "<h1 style='background:#FF9F00;border:0; color:black;\n",
    "    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n",
    "    transform: rotateX(10deg);\n",
    "    '><center style='color: #3E3D53;'>Data Preparation</center></h1>\n",
    "    \n",
    "# Data Prepartion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15b4c918",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T08:38:05.763733Z",
     "iopub.status.busy": "2024-06-10T08:38:05.762657Z",
     "iopub.status.idle": "2024-06-10T08:38:05.775544Z",
     "shell.execute_reply": "2024-06-10T08:38:05.774388Z"
    },
    "papermill": {
     "duration": 0.02212,
     "end_time": "2024-06-10T08:38:05.777908",
     "exception": false,
     "start_time": "2024-06-10T08:38:05.755788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    \"\"\"\n",
    "    A class for generating input and output examples for a character-level language model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        \"\"\"\n",
    "        Initializes a DataGenerator object.\n",
    "\n",
    "        Args:\n",
    "            path (str): The path to the text file containing the training data.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        \n",
    "        # Read in data from file and convert to lowercase\n",
    "        with open(path) as f:\n",
    "            data = f.read().lower()\n",
    "        \n",
    "        # Create list of unique characters in the data\n",
    "        self.chars = list(set(data))\n",
    "        \n",
    "        # Create dictionaries mapping characters to and from their index in the list of unique characters\n",
    "        self.char_to_idx = {ch: i for (i, ch) in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: ch for (i, ch) in enumerate(self.chars)}\n",
    "        \n",
    "        # Set the size of the vocabulary (i.e. number of unique characters)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        \n",
    "        # Read in examples from file and convert to lowercase, removing leading/trailing white space\n",
    "        with open(path) as f:\n",
    "            examples = f.readlines()\n",
    "        self.examples = [x.lower().strip() for x in examples]\n",
    " \n",
    "    def generate_example(self, idx):\n",
    "        \"\"\"\n",
    "        Generates an input/output example for the language model based on the given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the example to generate.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing the input and output arrays for the example.\n",
    "        \"\"\"\n",
    "        example_chars = self.examples[idx]\n",
    "        \n",
    "        # Convert the characters in the example to their corresponding indices in the list of unique characters\n",
    "        example_char_idx = [self.char_to_idx[char] for char in example_chars]\n",
    "        \n",
    "        # Add newline character as the first character in the input array, and as the last character in the output array\n",
    "        X = [self.char_to_idx['\\n']] + example_char_idx\n",
    "        Y = example_char_idx + [self.char_to_idx['\\n']]\n",
    "        \n",
    "        return np.array(X), np.array(Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead5cd08",
   "metadata": {
    "papermill": {
     "duration": 0.005175,
     "end_time": "2024-06-10T08:38:05.788587",
     "exception": false,
     "start_time": "2024-06-10T08:38:05.783412",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"3\"></a>\n",
    "<h1 style='background:#FF9F00;border:0; color:black;\n",
    "    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n",
    "    transform: rotateX(10deg);\n",
    "    '><center style='color: #3E3D53;'>RNN Implementation</center></h1>\n",
    "\n",
    "\n",
    "# RNN Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7540a0",
   "metadata": {
    "papermill": {
     "duration": 0.00496,
     "end_time": "2024-06-10T08:38:05.798908",
     "exception": false,
     "start_time": "2024-06-10T08:38:05.793948",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RNN  Implementation <a name=\"3-1\"></a>\n",
    "**The RNN used in this notebook is a basic one-layer RNN. It consists of an input layer, a hidden layer, and an output layer. The input layer takes in a one-hot encoded vector representing a character in the input sequence. This vector is multiplied by a weight matrix  $W_{ax}$ to produce a hidden state vector $a$. The hidden state vector is then passed through a non-linear activation function (in this case, the hyperbolic tangent function) and updated for each time step of the input sequence. The updated hidden state is then multiplied by a weight matrix  $W_{ya}$ to produce the output probability distribution over the next character in the sequence.**\n",
    "\n",
    "**The RNN is trained using stochastic gradient descent with the cross-entropy loss function. During training, the self takes in a sequence of characters and outputs the probability distribution over the next character. The true next character is then compared to the predicted probability distribution, and the parameters of the network are updated to minimize the cross-entropy loss.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ed9bb4",
   "metadata": {
    "papermill": {
     "duration": 0.005195,
     "end_time": "2024-06-10T08:38:05.809374",
     "exception": false,
     "start_time": "2024-06-10T08:38:05.804179",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Activation Functions\n",
    "### Softmax Activation Function\n",
    "\n",
    "**$$\\mathrm{softmax}(\\mathbf{x})_i = \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}}\n",
    "$$**\n",
    "\n",
    "**The softmax function is commonly used as an activation function in neural networks, particularly in the output layer for classification tasks. Given an input array $x$, the softmax function calculates the probability distribution of each element in the array**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Tanh Activation\n",
    "**$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$**\n",
    "\n",
    "**where $x$ is the input to the function. The output of the function is a value between -1 and 1. The tanh activation function is often used in neural networks as an alternative to the sigmoid activation function, as it has a steeper gradient and can better model non-linear relationships in the data.**\n",
    "****\n",
    "\n",
    "## Forward propagation:\n",
    "\n",
    "**During forward propagation, the input sequence is processed through the RNN to generate an output sequence. At each time step, the hidden state and the output are computed using the input, the previous hidden state, and the RNN's parameters.**\n",
    "\n",
    "**The equations for the forward propagation in a basic RNN are as follows:**\n",
    "\n",
    "**At time step $t$, the input to the RNN is $x_t$, and the hidden state at time step $t-1$ is $a_{t-1}$. The hidden state at time step $t$ is computed as:**\n",
    "\n",
    "**$a_t = \\tanh(W_{aa} a_{t-1} + W_{ax} x_t + b_a)$**\n",
    "\n",
    "**where $W_{aa}$ is the weight matrix for the hidden state, $W_{ax}$ is the weight matrix for the input, and $b_a$ is the bias vector for the hidden state.**\n",
    "\n",
    "**The output at time step $t$ is computed as:**\n",
    "\n",
    "**$y_t = softmax(W_{ya} a_t + b_y)$**\n",
    "\n",
    "**where $W_{ya}$ is the weight matrix for the output, and $b_y$ is the bias vector for the output.**\n",
    "****\n",
    "## Backward propagation:\n",
    "\n",
    "**The objective of training an RNN is to minimize the loss between the predicted sequence and the ground truth sequence. Backward propagation calculates the gradients of the loss with respect to the RNN's parameters, which are then used to update the parameters using an optimization algorithm such as Adagrad or Adam.**\n",
    "\n",
    "**The equations for the backward propagation in a basic RNN are as follows:**\n",
    "\n",
    "**At time step $t$, the loss with respect to the output $y_t$ is given by:**\n",
    "\n",
    "**$\\frac{\\partial L}{\\partial y_t} = -\\frac{1}{y_{t,i}} \\text{ if } i=t_i, \\text{ else } 0$**\n",
    "\n",
    "**where $L$ is the loss function, $y_{t,i}$ is the $i$th element of the output at time step $t$, and $t_i$ is the index of the true label at time step $t$**.\n",
    "\n",
    "**The loss with respect to the hidden state at time step $t$ is given by:**\n",
    "\n",
    "**$\\frac{\\partial L}{\\partial a_t} = \\frac{\\partial L}{\\partial y_t} W_{ya} + \\frac{\\partial L}{\\partial h_{t+1}} W_{aa}$**\n",
    "\n",
    "**where $\\frac{\\partial L}{\\partial a_{t+1}}$ is the gradient of the loss with respect to the hidden state at the next time step, which is backpropagated through time.**\n",
    "\n",
    "**The gradient with respect to tanh is given by:**\n",
    "**$\\frac{\\partial \\tanh(a)} {\\partial a}$**\n",
    "\n",
    "**The gradients with respect to the parameters are then computed using the chain rule:**\n",
    "\n",
    "**$\\frac{\\partial L}{\\partial W_{ya}} = \\sum_t \\frac{\\partial L}{\\partial y_t} a_t$**\n",
    "\n",
    "**$\\frac{\\partial L}{\\partial b_y} = \\sum_t \\frac{\\partial L}{\\partial y_t}$**\n",
    "\n",
    "**$\\frac{\\partial L}{\\partial W_{ax}} = \\sum_t \\frac{\\partial L}{\\partial a_t} \\frac{\\partial a_t}{\\partial W_{ax}}$**\n",
    "\n",
    "**$\\frac{\\partial L}{\\partial W_{aa}} = \\sum_t \\frac{\\partial L}{\\partial h_t} \\frac{\\partial h_t}{\\partial W_{aa}}$**\n",
    "\n",
    "**$\\frac{\\partial L}{\\partial b_a} = \\sum_t \\frac{\\partial L}{\\partial a_t} \\frac{\\partial h_t}{\\partial b_a}$**\n",
    "\n",
    "**where $\\frac{\\partial h_t}{\\partial W_{ax}}$, $\\frac{\\partial a_t}{\\partial W_{aa}}$, and $\\frac{\\partial h_t}{\\partial b_a}$ can be computed as:**\n",
    "\n",
    "**$\\frac{\\partial a_t}{\\partial W_{ax}} = x_t$**\n",
    "\n",
    "**$\\frac{\\partial a_t}{\\partial W_{aa}} = a_{t-1}$**\n",
    "\n",
    "**$\\frac{\\partial a_t}{\\partial b_a} = 1$**\n",
    "\n",
    "**These gradients are then used to update the parameters of the RNN using an optimization algorithm such as gradient descent, Adagrad, or Adam.**\n",
    "****\n",
    "## Loss:\n",
    "\n",
    "**The cross-entropy loss between the predicted probabilities y_pred and the true targets y_true at a single time step $t$ is:**\n",
    "\n",
    "**$$H(y_{true,t}, y_{pred,t}) = -\\sum_i y_{true,t,i} \\log(y_{pred,t,i})$$**\n",
    "\n",
    "**where $y_{pred,t}$ is the predicted probability distribution at time step $t$, $y_{true,t}$ is the true probability distribution at time step $t$ (i.e., a one-hot encoded vector representing the true target), and $i$ ranges over the vocabulary size.**\n",
    "\n",
    "**The total loss is then computed as the sum of the cross-entropy losses over all time steps:**\n",
    "\n",
    "**$$L = \\sum_{t=1}^{T} H(y_{true,t}, y_{pred,t})$$**\n",
    "\n",
    "**where $T$ is the sequence length.**\n",
    " \n",
    "****\n",
    "\n",
    "## Train:\n",
    "**The train method trains the RNN on a dataset using backpropagation through time. The method takes an instance of DataReader containing the training data as input. The method initializes a hidden state vector a_prev at the beginning of each sequence to zero. It then iterates until the smooth loss is less than a threshold value.**\n",
    "\n",
    "**During each iteration, it retrieves a batch of inputs and targets from the data reader. The RNN then performs a forward pass on the input sequence and computes the output probabilities. The backward pass is performed using the targets and output probabilities to calculate the gradients of the parameters of the network. The Adagrad algorithm is used to update the weights of the network.**\n",
    "\n",
    "**The method then calculates and updates the loss using the updated weights. The previous hidden state is updated for the next batch. The method prints the progress every 500 iterations by generating a sample of text using the sample method and printing the loss.**\n",
    "\n",
    "\n",
    "**The train method can be summarized by the following steps:**\n",
    "\n",
    " \n",
    "**$1.$ Initialize $a_{prev}$ to zero at the beginning of each sequence.**\n",
    "\n",
    "**$2.$ Retrieve a batch of inputs and targets from the data reader.**\n",
    "\n",
    "**$3.$ Perform a forward pass on the input sequence and compute the output probabilities.**\n",
    "\n",
    "**$4.$ Perform a backward pass using the targets and output probabilities to calculate the gradients of the parameters of the network.**\n",
    "\n",
    "**$5.$ Use the Adagrad algorithm to update the weights of the network.**\n",
    "\n",
    "**$6.$ Calculate and update the loss using the updated weights.**\n",
    "\n",
    "**$7.$ Update the previous hidden state for the next batch.**\n",
    "\n",
    "**$8.$ Print progress every 10000 iterations by generating a sample of text using the sample method and printing the loss.**\n",
    "\n",
    "**$9.$ Repeat steps $2$-$8$ until the smooth loss is less than the threshold value.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7179d5cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T08:38:05.822601Z",
     "iopub.status.busy": "2024-06-10T08:38:05.821624Z",
     "iopub.status.idle": "2024-06-10T08:38:05.882521Z",
     "shell.execute_reply": "2024-06-10T08:38:05.881590Z"
    },
    "papermill": {
     "duration": 0.070498,
     "end_time": "2024-06-10T08:38:05.885105",
     "exception": false,
     "start_time": "2024-06-10T08:38:05.814607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \"\"\"\n",
    "    A class used to represent a Recurrent Neural Network (RNN).\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    hidden_size : int\n",
    "        The number of hidden units in the RNN.\n",
    "    vocab_size : int\n",
    "        The size of the vocabulary used by the RNN.\n",
    "    sequence_length : int\n",
    "        The length of the input sequences fed to the RNN.\n",
    "    learning_rate : float\n",
    "        The learning rate used during training.\n",
    "    is_initialized : bool\n",
    "        Indicates whether the AdamW parameters has been initialized.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __init__(hidden_size, vocab_size, sequence_length, learning_rate)\n",
    "        Initializes an instance of the RNN class.\n",
    "    \n",
    "    forward(self, X, a_prev)\n",
    "     Computes the forward pass of the RNN.\n",
    "     \n",
    "    softmax(self, x)\n",
    "       Computes the softmax activation function for a given input array. \n",
    "       \n",
    "    backward(self,x, a, y_preds, targets)    \n",
    "        Implements the backward pass of the RNN.\n",
    "        \n",
    "   loss(self, y_preds, targets)\n",
    "     Computes the cross-entropy loss for a given sequence of predicted probabilities and true targets. \n",
    "     \n",
    "    adamw(self, beta1=0.9, beta2=0.999, epsilon=1e-8, L2_reg=1e-4)\n",
    "       Updates the RNN's parameters using the AdamW optimization algorithm.\n",
    "       \n",
    "    train(self, generated_names=5)\n",
    "       Trains the RNN on a dataset using backpropagation through time (BPTT).   \n",
    "       \n",
    "   predict(self, start)\n",
    "        Generates a sequence of characters using the trained self, starting from the given start sequence.\n",
    "        The generated sequence may contain a maximum of 50 characters or a newline character.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, data_generator, sequence_length, learning_rate):\n",
    "        \"\"\"\n",
    "        Initializes an instance of the RNN class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hidden_size : int\n",
    "            The number of hidden units in the RNN.\n",
    "        vocab_size : int\n",
    "            The size of the vocabulary used by the RNN.\n",
    "        sequence_length : int\n",
    "            The length of the input sequences fed to the RNN.\n",
    "        learning_rate : float\n",
    "            The learning rate used during training.\n",
    "        \"\"\"\n",
    "\n",
    "        # hyper parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.data_generator = data_generator\n",
    "        self.vocab_size = self.data_generator.vocab_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.learning_rate = learning_rate\n",
    "        self.X = None\n",
    "\n",
    "        # model parameters\n",
    "        self.Wax = np.random.uniform(-np.sqrt(1. / self.vocab_size), np.sqrt(1. / self.vocab_size), (hidden_size, self.vocab_size))\n",
    "        self.Waa = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size), (hidden_size, hidden_size))\n",
    "        self.Wya = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size), (self.vocab_size, hidden_size))\n",
    "        self.ba = np.zeros((hidden_size, 1))  \n",
    "        self.by = np.zeros((self.vocab_size, 1))\n",
    "        \n",
    "        # Initialize gradients\n",
    "        self.dWax, self.dWaa, self.dWya = np.zeros_like(self.Wax), np.zeros_like(self.Waa), np.zeros_like(self.Wya)\n",
    "        self.dba, self.dby = np.zeros_like(self.ba), np.zeros_like(self.by)\n",
    "        \n",
    "        # parameter update with AdamW\n",
    "        self.mWax = np.zeros_like(self.Wax)\n",
    "        self.vWax = np.zeros_like(self.Wax)\n",
    "        self.mWaa = np.zeros_like(self.Waa)\n",
    "        self.vWaa = np.zeros_like(self.Waa)\n",
    "        self.mWya = np.zeros_like(self.Wya)\n",
    "        self.vWya = np.zeros_like(self.Wya)\n",
    "        self.mba = np.zeros_like(self.ba)\n",
    "        self.vba = np.zeros_like(self.ba)\n",
    "        self.mby = np.zeros_like(self.by)\n",
    "        self.vby = np.zeros_like(self.by)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Computes the softmax activation function for a given input array.\n",
    "\n",
    "        Parameters:\n",
    "            x (ndarray): Input array.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Array of the same shape as `x`, containing the softmax activation values.\n",
    "        \"\"\"\n",
    "        # shift the input to prevent overflow when computing the exponentials\n",
    "        x = x - np.max(x)\n",
    "        # compute the exponentials of the shifted input\n",
    "        p = np.exp(x)\n",
    "        # normalize the exponentials by dividing by their sum\n",
    "        return p / np.sum(p)\n",
    "\n",
    "    def forward(self, X, a_prev):\n",
    "        \"\"\"\n",
    "        Compute the forward pass of the RNN.\n",
    "\n",
    "        Parameters:\n",
    "        X (ndarray): Input data of shape (seq_length, vocab_size)\n",
    "        a_prev (ndarray): Activation of the previous time step of shape (hidden_size, 1)\n",
    "\n",
    "        Returns:\n",
    "        x (dict): Dictionary of input data of shape (seq_length, vocab_size, 1), with keys from 0 to seq_length-1\n",
    "        a (dict): Dictionary of hidden activations for each time step, with keys from 0 to seq_length-1\n",
    "        y_pred (dict): Dictionary of output probabilities for each time step, with keys from 0 to seq_length-1\n",
    "        \"\"\"\n",
    "        # Initialize dictionaries to store activations and output probabilities.\n",
    "        x, a, y_pred = {}, {}, {}\n",
    "\n",
    "        # Store the input data in the class variable for later use in the backward pass.\n",
    "        self.X = X\n",
    "\n",
    "        # Set the initial activation to the previous activation.\n",
    "        a[-1] = np.copy(a_prev)\n",
    "        # iterate over each time step in the input sequence\n",
    "        for t in range(len(self.X)): \n",
    "            # get the input at the current time step\n",
    "            x[t] = np.zeros((self.vocab_size,1)) \n",
    "            if (self.X[t] != None):\n",
    "                x[t][self.X[t]] = 1\n",
    "            # compute the hidden activation at the current time step\n",
    "            a[t] = np.tanh(np.dot(self.Wax, x[t]) + np.dot(self.Waa, a[t - 1]) + self.ba)\n",
    "            # compute the output probabilities at the current time step\n",
    "            y_pred[t] = self.softmax(np.dot(self.Wya, a[t]) + self.by)\n",
    "            # add an extra dimension to X to make it compatible with the shape of the input to the backward pass\n",
    "         # return the input, hidden activations, and output probabilities at each time step\n",
    "        return x, a, y_pred \n",
    "    \n",
    "    def backward(self,x, a, y_preds, targets):\n",
    "        \"\"\"\n",
    "        Implement the backward pass of the RNN.\n",
    "\n",
    "        Args:\n",
    "        x -- (dict) of input characters (as one-hot encoding vectors) for each time-step, shape (vocab_size, sequence_length)\n",
    "        a -- (dict) of hidden state vectors for each time-step, shape (hidden_size, sequence_length)\n",
    "        y_preds -- (dict) of output probability vectors (after softmax) for each time-step, shape (vocab_size, sequence_length)\n",
    "        targets -- (list) of integer target characters (indices of characters in the vocabulary) for each time-step, shape (1, sequence_length)\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "\n",
    "        \"\"\"\n",
    "        # Initialize derivative of hidden state for the last time-step\n",
    "        da_next = np.zeros_like(a[0])\n",
    "\n",
    "        # Loop through the input sequence backwards\n",
    "        for t in reversed(range(len(self.X))):\n",
    "            # Calculate derivative of output probability vector\n",
    "            dy_preds = np.copy(y_preds[t])\n",
    "            dy_preds[targets[t]] -= 1\n",
    "\n",
    "            # Calculate derivative of hidden state\n",
    "            da = np.dot(self.Waa.T, da_next) + np.dot(self.Wya.T, dy_preds)\n",
    "            dtanh = (1 - np.power(a[t], 2))\n",
    "            da_unactivated = dtanh * da\n",
    "\n",
    "            # Calculate gradients\n",
    "            self.dba += da_unactivated\n",
    "            self.dWax += np.dot(da_unactivated, x[t].T)\n",
    "            self.dWaa += np.dot(da_unactivated, a[t - 1].T)\n",
    "\n",
    "            # Update derivative of hidden state for the next iteration\n",
    "            da_next = da_unactivated\n",
    "\n",
    "            # Calculate gradient for output weight matrix\n",
    "            self.dWya += np.dot(dy_preds, a[t].T)\n",
    "\n",
    "            # clip gradients to avoid exploding gradients\n",
    "            for grad in [self.dWax, self.dWaa, self.dWya, self.dba, self.dby]:\n",
    "                np.clip(grad, -1, 1, out=grad)\n",
    " \n",
    "    def loss(self, y_preds, targets):\n",
    "        \"\"\"\n",
    "        Computes the cross-entropy loss for a given sequence of predicted probabilities and true targets.\n",
    "\n",
    "        Parameters:\n",
    "            y_preds (ndarray): Array of shape (sequence_length, vocab_size) containing the predicted probabilities for each time step.\n",
    "            targets (ndarray): Array of shape (sequence_length, 1) containing the true targets for each time step.\n",
    "\n",
    "        Returns:\n",
    "            float: Cross-entropy loss.\n",
    "        \"\"\"\n",
    "        # calculate cross-entropy loss\n",
    "        return sum(-np.log(y_preds[t][targets[t], 0]) for t in range(len(self.X)))\n",
    "    \n",
    "    def adamw(self, beta1=0.9, beta2=0.999, epsilon=1e-8, L2_reg=1e-4):\n",
    "        \"\"\"\n",
    "        Updates the RNN's parameters using the AdamW optimization algorithm.\n",
    "        \"\"\"\n",
    "        # AdamW update for Wax\n",
    "        self.mWax = beta1 * self.mWax + (1 - beta1) * self.dWax\n",
    "        self.vWax = beta2 * self.vWax + (1 - beta2) * np.square(self.dWax)\n",
    "        m_hat = self.mWax / (1 - beta1)\n",
    "        v_hat = self.vWax / (1 - beta2)\n",
    "        self.Wax -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wax)\n",
    "\n",
    "        # AdamW update for Waa\n",
    "        self.mWaa = beta1 * self.mWaa + (1 - beta1) * self.dWaa\n",
    "        self.vWaa = beta2 * self.vWaa + (1 - beta2) * np.square(self.dWaa)\n",
    "        m_hat = self.mWaa / (1 - beta1)\n",
    "        v_hat = self.vWaa / (1 - beta2)\n",
    "        self.Waa -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Waa)\n",
    "\n",
    "        # AdamW update for Wya\n",
    "        self.mWya = beta1 * self.mWya + (1 - beta1) * self.dWya\n",
    "        self.vWya = beta2 * self.vWya + (1 - beta2) * np.square(self.dWya)\n",
    "        m_hat = self.mWya / (1 - beta1)\n",
    "        v_hat = self.vWya / (1 - beta2)\n",
    "        self.Wya -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wya)\n",
    "\n",
    "        # AdamW update for ba\n",
    "        self.mba = beta1 * self.mba + (1 - beta1) * self.dba\n",
    "        self.vba = beta2 * self.vba + (1 - beta2) * np.square(self.dba)\n",
    "        m_hat = self.mba / (1 - beta1)\n",
    "        v_hat = self.vba / (1 - beta2)\n",
    "        self.ba -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.ba)\n",
    "\n",
    "        # AdamW update for by\n",
    "        self.mby = beta1 * self.mby + (1 - beta1) * self.dby\n",
    "        self.vby = beta2 * self.vby + (1 - beta2) * np.square(self.dby)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Sample a sequence of characters from the RNN.\n",
    "\n",
    "        Args:\n",
    "            None\n",
    "\n",
    "        Returns:\n",
    "            list: A list of integers representing the generated sequence.\n",
    "        \"\"\"\n",
    "        # initialize input and hidden state\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        a_prev = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        # create an empty list to store the generated character indices\n",
    "        indices = []\n",
    "\n",
    "        # idx is a flag to detect a newline character, initialize it to -1\n",
    "        idx = -1\n",
    "\n",
    "        # generate sequence of characters\n",
    "        counter = 0\n",
    "        max_chars = 50 # maximum number of characters to generate\n",
    "        newline_character = self.data_generator.char_to_idx['\\n'] # the newline character\n",
    "\n",
    "        while (idx != newline_character and counter != max_chars):\n",
    "            # compute the hidden state\n",
    "            a = np.tanh(np.dot(self.Wax, x) + np.dot(self.Waa, a_prev) + self.ba)\n",
    "\n",
    "            # compute the output probabilities\n",
    "            y = self.softmax(np.dot(self.Wya, a) + self.by)\n",
    "\n",
    "            # sample the next character from the output probabilities\n",
    "            idx = np.random.choice(list(range(self.vocab_size)), p=y.ravel())\n",
    "\n",
    "            # set the input for the next time step\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "\n",
    "            # store the sampled character index in the list\n",
    "            indices.append(idx)\n",
    "\n",
    "            # update the previous hidden state\n",
    "            a_prev = a\n",
    "\n",
    "            # increment the counter\n",
    "            counter += 1\n",
    "\n",
    "        # return the list of sampled character indices\n",
    "        return indices\n",
    "\n",
    "        \n",
    "    def train(self, generated_names=5):\n",
    "        \"\"\"\n",
    "        Train the RNN on a dataset using backpropagation through time (BPTT).\n",
    "\n",
    "        Args:\n",
    "        - generated_names: an integer indicating how many example names to generate during training.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "\n",
    "        iter_num = 0\n",
    "        threshold = 5 # stopping criterion for training\n",
    "        smooth_loss = -np.log(1.0 / self.data_generator.vocab_size) * self.sequence_length  # initialize loss\n",
    "\n",
    "        while (smooth_loss > threshold):\n",
    "            a_prev = np.zeros((self.hidden_size, 1))\n",
    "            idx = iter_num % self.vocab_size\n",
    "            # get a batch of inputs and targets\n",
    "            inputs, targets = self.data_generator.generate_example(idx)\n",
    "\n",
    "            # forward pass\n",
    "            x, a, y_pred  = self.forward(inputs, a_prev)\n",
    "\n",
    "            # backward pass\n",
    "            self.backward(x, a, y_pred, targets)\n",
    "\n",
    "            # calculate and update loss\n",
    "            loss = self.loss(y_pred, targets)\n",
    "            self.adamw()\n",
    "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "            # update previous hidden state for the next batch\n",
    "            a_prev = a[len(self.X) - 1]\n",
    "            # print progress every 500 iterations\n",
    "            if iter_num % 500 == 0:\n",
    "                print(\"\\n\\niter :%d, loss:%f\\n\" % (iter_num, smooth_loss))\n",
    "                for i in range(generated_names):\n",
    "                    sample_idx = self.sample()\n",
    "                    txt = ''.join(self.data_generator.idx_to_char[idx] for idx in sample_idx)\n",
    "                    txt = txt.title()  # capitalize first character \n",
    "                    print ('%s' % (txt, ), end='')\n",
    "            iter_num += 1\n",
    "    \n",
    "    def predict(self, start):\n",
    "        \"\"\"\n",
    "        Generate a sequence of characters using the trained self, starting from the given start sequence.\n",
    "        The generated sequence may contain a maximum of 50 characters or a newline character.\n",
    "\n",
    "        Args:\n",
    "        - start: a string containing the start sequence\n",
    "\n",
    "        Returns:\n",
    "        - txt: a string containing the generated sequence\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize input vector and previous hidden state\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        a_prev = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        # Convert start sequence to indices\n",
    "        chars = [ch for ch in start]\n",
    "        idxes = []\n",
    "        for i in range(len(chars)):\n",
    "            idx = self.data_generator.char_to_idx[chars[i]]\n",
    "            x[idx] = 1\n",
    "            idxes.append(idx)\n",
    "\n",
    "        # Generate sequence\n",
    "        max_chars = 50  # maximum number of characters to generate\n",
    "        newline_character = self.data_generator.char_to_idx['\\n']  # the newline character\n",
    "        counter = 0\n",
    "        while (idx != newline_character and counter != max_chars):\n",
    "            # Compute next hidden state and predicted character\n",
    "            a = np.tanh(np.dot(self.Wax, x) + np.dot(self.Waa, a_prev) + self.ba)\n",
    "            y_pred = self.softmax(np.dot(self.Wya, a) + self.by)\n",
    "            idx = np.random.choice(range(self.vocab_size), p=y_pred.ravel())\n",
    "\n",
    "            # Update input vector, previous hidden state, and indices\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "            a_prev = a\n",
    "            idxes.append(idx)\n",
    "            counter += 1\n",
    "\n",
    "        # Convert indices to characters and concatenate into a string\n",
    "        txt = ''.join(self.data_generator.idx_to_char[i] for i in idxes)\n",
    "\n",
    "        # Remove newline character if it exists at the end of the generated sequence\n",
    "        if txt[-1] == '\\n':\n",
    "            txt = txt[:-1]\n",
    "\n",
    "        return txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50c13c7a",
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-10T08:38:05.897598Z",
     "iopub.status.busy": "2024-06-10T08:38:05.897200Z",
     "iopub.status.idle": "2024-06-10T08:39:17.622546Z",
     "shell.execute_reply": "2024-06-10T08:39:17.620976Z"
    },
    "papermill": {
     "duration": 71.736781,
     "end_time": "2024-06-10T08:39:17.627430",
     "exception": false,
     "start_time": "2024-06-10T08:38:05.890649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iter :0, loss:82.359627\n",
      "\n",
      "Pirtnldgf\n",
      "Mqtxaylxqzchqeeeynpgittgp\n",
      "Qhunojcwowrqtutwrqwfwitsolfxha\n",
      "Qeogjotwaqrwpaxs\n",
      "Bwucquqjnrqwwgtyma\n",
      "\n",
      "\n",
      "iter :500, loss:58.811628\n",
      "\n",
      "Alchalosasrus\n",
      "Aarauth\n",
      "Mar\n",
      "Hclixhxaurus\n",
      "Acdistraumumurus\n",
      "\n",
      "\n",
      "iter :1000, loss:41.035397\n",
      "\n",
      "Achiplosfumus\n",
      "Alristhonhomumus\n",
      "Ahrlosaurus\n",
      "Atelosousus\n",
      "Vcyyolosaurus\n",
      "\n",
      "\n",
      "iter :1500, loss:28.692491\n",
      "\n",
      "Afrisaurus\n",
      "Aegyctosaur\n",
      "Xaanosaprus\n",
      "Actnrnoharus\n",
      "Ademisaurus\n",
      "\n",
      "\n",
      "iter :2000, loss:20.303059\n",
      "\n",
      "Aegynyxafromimus\n",
      "Achilhelahenhsaurus\n",
      "Acanthnthosaurus\n",
      "Icanthophsaurus\n",
      "Achopyaphosaurus\n",
      "\n",
      "\n",
      "iter :2500, loss:14.719314\n",
      "\n",
      "Popasteosaurus\n",
      "Asropnobausaurus\n",
      "Acrorapthopisaurus\n",
      "Afroranthosaurus\n",
      "Dapyptosaurus\n",
      "\n",
      "\n",
      "iter :3000, loss:11.076720\n",
      "\n",
      "Acrosaurus\n",
      "Achisaurus\n",
      "Ahelosaurus\n",
      "Aathosafrumimus\n",
      "Acrostusaurus\n",
      "\n",
      "\n",
      "iter :3500, loss:8.693048\n",
      "\n",
      "Afrovenator\n",
      "Afrovanthosaurus\n",
      "Ahelisaurus\n",
      "Aeolosaurus\n",
      "Acanthophomimus\n",
      "\n",
      "\n",
      "iter :4000, loss:7.143386\n",
      "\n",
      "Achetorop\n",
      "Actiosaurus\n",
      "Acrorax\n",
      "Actiosaurus\n",
      "Adelausaurus\n",
      "\n",
      "\n",
      "iter :4500, loss:6.148270\n",
      "\n",
      "Acristeopholistiousourus\n",
      "Acropaoporosturus\n",
      "Piteoloptopusturus\n",
      "Acamantisaurus\n",
      "Attelotholus\n",
      "\n",
      "\n",
      "iter :5000, loss:5.519813\n",
      "\n",
      "Aaelosaurus\n",
      "Afrovenator\n",
      "Aoristevesaurus\n",
      "Ahamcavenyxafrosaurus\n",
      "Attdosaurus\n",
      "\n",
      "\n",
      "iter :5500, loss:5.092536\n",
      "\n",
      "Afrovenyx\n",
      "Afrovenotor\n",
      "Aetonyxafhomimus\n",
      "Aorostnolophus\n",
      "Afrovenator\n"
     ]
    }
   ],
   "source": [
    "data_generator = DataGenerator('/kaggle/input/dinosaur-island/dinos.txt')\n",
    "rnn = RNN(hidden_size=200,data_generator=data_generator, sequence_length=25, learning_rate=1e-3)\n",
    "rnn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7eb8f45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T08:39:17.658629Z",
     "iopub.status.busy": "2024-06-10T08:39:17.657941Z",
     "iopub.status.idle": "2024-06-10T08:39:17.681152Z",
     "shell.execute_reply": "2024-06-10T08:39:17.679628Z"
    },
    "papermill": {
     "duration": 0.044215,
     "end_time": "2024-06-10T08:39:17.685961",
     "exception": false,
     "start_time": "2024-06-10T08:39:17.641746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meonausaurus'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.predict(\"meo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ccb9aa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T08:39:17.717781Z",
     "iopub.status.busy": "2024-06-10T08:39:17.717128Z",
     "iopub.status.idle": "2024-06-10T08:39:17.734814Z",
     "shell.execute_reply": "2024-06-10T08:39:17.733360Z"
    },
    "papermill": {
     "duration": 0.041559,
     "end_time": "2024-06-10T08:39:17.742542",
     "exception": false,
     "start_time": "2024-06-10T08:39:17.700983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'audastaoros'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.predict(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9110b30",
   "metadata": {
    "papermill": {
     "duration": 0.014083,
     "end_time": "2024-06-10T08:39:17.771274",
     "exception": false,
     "start_time": "2024-06-10T08:39:17.757191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa3a7cc",
   "metadata": {
    "papermill": {
     "duration": 0.013997,
     "end_time": "2024-06-10T08:39:17.799951",
     "exception": false,
     "start_time": "2024-06-10T08:39:17.785954",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"4\"></a>\n",
    "<h1 style='background:#FF9F00;border:0; color:black;\n",
    "    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n",
    "    transform: rotateX(10deg);\n",
    "    '><center style='color: #3E3D53;'>Thank you</center></h1>\n",
    "\n",
    "# Thank you\n",
    "\n",
    "**Thank you for going through this notebook**\n",
    "\n",
    "**If you have any suggestions please let me know**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 81348,
     "sourceId": 189031,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 75.471987,
   "end_time": "2024-06-10T08:39:18.143183",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-10T08:38:02.671196",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
